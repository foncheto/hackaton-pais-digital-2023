{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capítulo IV: Memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La forma más simple de almacenar la información de conversaciones es mediante la siguiente sintaxis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "memory.chat_memory.add_user_message(\"Hola!\")\n",
    "memory.chat_memory.add_ai_message(\"Cómo va todo?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Hola!\\nAI: Cómo va todo?'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({}) #<-- Notar que aquí el historial se está alcenando en el key \"history\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Es posible que este historial se pueda alojar en otra key, para la manipulación posterior de la cadena\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "memory.chat_memory.add_user_message(\"Hola!\")\n",
    "memory.chat_memory.add_ai_message(\"Cómo va todo?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': 'Human: Hola!\\nAI: Cómo va todo?'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#También, puedo manipular que la salida de mensajes sea en formato lista para mayor conveniencia hacia ciertos modelos (como los modelos de chat)\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "memory.chat_memory.add_user_message(\"Hola!\")\n",
    "memory.chat_memory.add_ai_message(\"Cómo va todo?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hola!', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='Cómo va todo?', additional_kwargs={}, example=False)]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Como se puede ver, las respuestas retornan múltiples keys. En proceso contínuo conversacional, para acceder a aquellos elementos clave hay que prestar atención a las keys \"input_key\" y \"output_key\"\n",
    "#Por defecto son None, y si no hay más de una interacción permanecen con ese valor\n",
    "\n",
    "#Ejemplo usando un ChatModel\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"Eres un chatbot agradable que mantiene una conversación con un humano\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"), # El término \"variable_name\" debe estar en línea con la memoria\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True) #<-- # \"chat_history\" debe coincidir con \"MessagesPlaceholder()\"\n",
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Eres un chatbot agradable que mantiene una conversación con un humano\n",
      "Human: hola\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'hola',\n",
       " 'chat_history': [HumanMessage(content='hola', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='¡Hola! ¿Cómo estás? Soy un chatbot amigable, ¿en qué puedo ayudarte hoy?', additional_kwargs={}, example=False)],\n",
       " 'text': '¡Hola! ¿Cómo estás? Soy un chatbot amigable, ¿en qué puedo ayudarte hoy?'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation({\"question\": \"hola\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Eres un chatbot agradable que mantiene una conversación con un humano\n",
      "Human: hola\n",
      "AI: ¡Hola! ¿Cómo estás? Soy un chatbot amigable, ¿en qué puedo ayudarte hoy?\n",
      "Human: conoces de algún buen libro de poesía latinoamerica?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'conoces de algún buen libro de poesía latinoamerica?',\n",
       " 'chat_history': [HumanMessage(content='hola', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='¡Hola! ¿Cómo estás? Soy un chatbot amigable, ¿en qué puedo ayudarte hoy?', additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='conoces de algún buen libro de poesía latinoamerica?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='¡Claro! Hay muchos libros de poesía latinoamericana maravillosos para recomendar. Aquí te dejo algunas opciones:\\n\\n1. \"Veinte poemas de amor y una canción desesperada\" de Pablo Neruda (Chile)\\n2. \"Altazor o el viaje en paracaídas\" de Vicente Huidobro (Chile)\\n3. \"Cantos de vida y esperanza\" de Rubén Darío (Nicaragua)\\n4. \"Poesía completa\" de Octavio Paz (México)\\n5. \"Los heraldos negros\" de César Vallejo (Perú)\\n6. \"El gran Burundún-Burundá ha muerto\" de Roque Dalton (El Salvador)\\n7. \"Poesía completa\" de Mario Benedetti (Uruguay)\\n8. \"El libro de los abrazos\" de Eduardo Galeano (Uruguay)\\n9. \"Casa tomada\" de Julio Cortázar (Argentina)\\n10. \"La casa de los espíritus\" de Isabel Allende (Chile)\\n\\nEspero que encuentres alguna de estas opciones interesantes. ¡Disfruta de la lectura!', additional_kwargs={}, example=False)],\n",
       " 'text': '¡Claro! Hay muchos libros de poesía latinoamericana maravillosos para recomendar. Aquí te dejo algunas opciones:\\n\\n1. \"Veinte poemas de amor y una canción desesperada\" de Pablo Neruda (Chile)\\n2. \"Altazor o el viaje en paracaídas\" de Vicente Huidobro (Chile)\\n3. \"Cantos de vida y esperanza\" de Rubén Darío (Nicaragua)\\n4. \"Poesía completa\" de Octavio Paz (México)\\n5. \"Los heraldos negros\" de César Vallejo (Perú)\\n6. \"El gran Burundún-Burundá ha muerto\" de Roque Dalton (El Salvador)\\n7. \"Poesía completa\" de Mario Benedetti (Uruguay)\\n8. \"El libro de los abrazos\" de Eduardo Galeano (Uruguay)\\n9. \"Casa tomada\" de Julio Cortázar (Argentina)\\n10. \"La casa de los espíritus\" de Isabel Allende (Chile)\\n\\nEspero que encuentres alguna de estas opciones interesantes. ¡Disfruta de la lectura!'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation({\"question\": \"conoces de algún buen libro de poesía latinoamerica?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tipos de Memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversation Buffer: Esta memoria permite almacenar y extraer mensajes en una variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    verbose=True, \n",
    "    memory=ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hola!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Hola! ¿Cómo estás? (Hello! How are you?)'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hola!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hola!\n",
      "AI:  Hola! ¿Cómo estás? (Hello! How are you?)\n",
      "Human: Todo bien, preparando una charla sobre Modelos de Lenguaje\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" ¡Qué interesante! ¿De qué trata exactamente tu charla? (That's interesting! What is your talk about exactly?)\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Todo bien, preparando una charla sobre Modelos de Lenguaje\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversation Buffer Window: mantiene una lista de las interacciones de la conversación a lo largo del tiempo. Sólo utiliza las últimas K interacciones. Esto puede resultar útil para mantener una ventana deslizante de las interacciones más recientes, de modo que el búfer no crezca demasiado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hola!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Hola! ¿Cómo estás? (Hello! How are you?)'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=OpenAI(temperature=0), \n",
    "    memory=ConversationBufferWindowMemory(k=2), #<-- k=2, solo conserva las últimas 2 interactions en memoria\n",
    "    verbose=True\n",
    ")\n",
    "conversation_with_summary.predict(input=\"Hola!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hola!\n",
      "AI:  Hola! ¿Cómo estás? (Hello! How are you?)\n",
      "Human: Todo bien, preparando una charla sobre Modelos de Lenguaje\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" ¡Qué interesante! ¿De qué trata exactamente tu charla? (That's interesting! What is your talk about exactly?)\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"Todo bien, preparando una charla sobre Modelos de Lenguaje\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hola!\n",
      "AI:  Hola! ¿Cómo estás? (Hello! How are you?)\n",
      "Human: Todo bien, preparando una charla sobre Modelos de Lenguaje\n",
      "AI:  ¡Qué interesante! ¿De qué trata exactamente tu charla? (That's interesting! What is your talk about exactly?)\n",
      "Human: Es sobre Inteligencia Artificial\n",
      "AI:\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" ¡Ah, qué interesante! ¿Qué aspectos de la Inteligencia Artificial abordarás en tu charla? (Oh, that's interesting! What aspects of Artificial Intelligence will you cover in your talk?)\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"Es sobre Inteligencia Artificial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Todo bien, preparando una charla sobre Modelos de Lenguaje\n",
      "AI:  ¡Qué interesante! ¿De qué trata exactamente tu charla? (That's interesting! What is your talk about exactly?)\n",
      "Human: Es sobre Inteligencia Artificial\n",
      "AI:  ¡Ah, qué interesante! ¿Qué aspectos de la Inteligencia Artificial abordarás en tu charla? (Oh, that's interesting! What aspects of Artificial Intelligence will you cover in your talk?)\n",
      "Human: Sobre Modelos de Lenguaje\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' ¡Comprendo! ¿Qué tipo de modelos de lenguaje abordarás en tu charla? ¿Incluirás modelos de lenguaje basados en redes neuronales, modelos de lenguaje basados en reglas, modelos de lenguaje basados en lógica, o algo más? (I see! What kind of language models will you cover in your talk? Will you include neural network-based language models, rule-based language models, logic-based language models, or something else?)'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"Sobre Modelos de Lenguaje\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
